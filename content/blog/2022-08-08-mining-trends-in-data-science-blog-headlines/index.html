---
title: Mining Trends in Data Science Blog Headlines - Synonym Extraction with Shallow Transfer Learning from CommonSense Knowlege graph embeddings and Fasttext embeddings pretrained on 1.3 billion Google Queries  
author: Knut Jägersberg
date: '2022-08-08'
slug: []
categories:
  - Data Science
tags: []
---

<script src="{{< blogdown/postref >}}index_files/htmlwidgets/htmlwidgets.js"></script>
<link href="{{< blogdown/postref >}}index_files/vis/vis-network.min.css" rel="stylesheet" />
<script src="{{< blogdown/postref >}}index_files/vis/vis-network.min.js"></script>
<script src="{{< blogdown/postref >}}index_files/visNetwork-binding/visNetwork.js"></script>


<div id="mining-trends-in-data-science-blog-headlines---spawn-selectors" class="section level1">
<h1>Mining Trends in Data Science Blog Headlines - Spawn Selectors</h1>
<p>We’ll look at how to mine blog posts for learning about influence vectors of the data science bubble. A workflow for labeling text data in an ML supported way. It is based on the same process used for writing dictionaries in dictionary based labeling approaches and is utilizes semantic folding. Semantic folding is the utilization of self-supervised learning and massive pretrained models to journey through semantic space at superlogical velocity. We use model-intersubjectivity as gravity drive. Centering content around model core concepts which are related to one another but are from different perspectives yields semantic singularity: One models worldview is the dialectics of anothers ignorance. These intersubjective wormholes, a form of analogical reasoning, allow us to reason faster than possible by the rules of formal logic, in the realms of overlooked validity.<br />
Whilst we can do this across pretrained models from different domains to move around points of interest (i.e. warp Venus to the edge of Andromeda’s black hole and an earth alike planet to its place), there is more syntropy in defaulting to the language model in our head.</p>
<pre class="r"><code>blogdown::shortcode(&quot;youtube&quot;, &quot;q5JJGX-Wppk&quot;)</code></pre>
<p>{{% youtube "q5JJGX-Wppk" %}}</p>
</div>
<div id="extracting-a-selector-graph-for-data-science-blog-post-titles" class="section level1">
<h1>Extracting a selector graph for data science blog post titles</h1>
<p>Peek into the scraped file:</p>
<pre class="r"><code>pacman::p_load(tidyverse, tidytable, data.table, tidytext, textrank, job, UBL, archetypes, future, future.callr, genieclust, doc2vec)


blogposts &lt;- fread(&quot;/home/knut/Desktop/my_niche/data/twds_scrape_distinct.csv&quot;)


blogposts[!duplicated(blogposts$variable),]</code></pre>
<pre><code>##           variable
##  1:         author
##  2:   author_motto
##  3: follower_count
##  4:          claps
##  5:           date
##  6:   reading_time
##  7:          title
##  8:       subtitle
##  9:             h1
## 10:        bullets
## 11:     paragraphs
## 12:           body
##                                                             value
##  1:                                                              
##  2:                                                       ·Feb 14
##  3:                                                 81K Followers
##  4:                                                              
##  5:                                                        Feb 14
##  6:                                                   21 min read
##  7: Stats Gist List: An Irreverent Statistician’s Guide to Jargon
##  8:                                                              
##  9: Stats Gist List: An Irreverent Statistician’s Guide to Jargon
## 10:                                                              
## 11:                                                       Sign In
## 12:</code></pre>
<p>Taking the number of titles as indication, this file contains 25k blog posts.</p>
<pre class="r"><code>titles &lt;- blogposts %&gt;% filter.(variable==&quot;title&quot;) %&gt;% distinct(value)
titles %&gt;% nrow()</code></pre>
<pre><code>## [1] 25240</code></pre>
<p>First, let’s extract keywords and keyphrases.</p>
<pre class="r"><code>#extract global keywords


# f &lt;- callr({
#   library(spacyr)
# 
#   
#   spacy_initialize(model = &quot;en_core_web_trf&quot;, python_executable = &quot;/home/knut/transformers/bin/python3&quot;)
# 
#   tags &lt;- spacy_parse(titles$value, entity = T, nounphrase = T)
#   
#   
# }, workers=1)


# pos_tags &lt;- value(f)
pos_tags &lt;- fst::read_fst(&quot;/home/knut/Desktop/pos_tags.fst&quot;)


keyw_nouns &lt;- textrank_keywords(pos_tags$lemma %&gt;% tolower(),
                          relevant = pos_tags$pos %in% c(&quot;NOUN&quot;),)
keyw_nouns &lt;- subset(keyw_nouns$keywords, ngram &gt; 0 &amp; freq &gt; 1)%&gt;% mutate(title=str_replace_all(keyword, &quot;-&quot;, &quot; &quot;))

keyw_nouns$title &lt;- str_squish(keyw_nouns$title)



keyw_verbs &lt;- textrank_keywords(pos_tags$lemma %&gt;% tolower(),
                          relevant = pos_tags$pos %in% c(&quot;VERB&quot;),)
keyw_verbs &lt;- subset(keyw_verbs$keywords, ngram &gt; 0 &amp; freq &gt; 1)%&gt;% mutate(title=str_replace_all(keyword, &quot;-&quot;, &quot; &quot;))

keyw_verbs$title &lt;- str_squish(keyw_verbs$title)


keyw_adjective &lt;- textrank_keywords(pos_tags$lemma %&gt;% tolower(),
                          relevant = pos_tags$pos %in% c(&quot;ADJ&quot;),)
keyw_adjective &lt;- subset(keyw_adjective$keywords, ngram &gt; 0 &amp; freq &gt; 1)%&gt;% mutate(title=str_replace_all(keyword, &quot;-&quot;, &quot; &quot;))

keyw_adjective$title &lt;- str_squish(keyw_adjective$title)



#extract local keywords


# job({
#   
#   library(tidyverse)
#   library(reticulate)
#   library(purrrgress)
#   use_python(&quot;/home/knut/transformers/bin/python3&quot;, required = T)
#   
#   kb &lt;- import(&quot;keybert&quot;)
#   
#   
#   kw_model &lt;- kb$KeyBERT(model = &#39;all-distilroberta-v1&#39;)
#   
# 
#   similar &lt;- function(doc){
#   kw_model$extract_keywords(doc, keyphrase_ngram_range=list(as.integer(1L), as.integer(3L)), use_mmr = T, diversity=as.numeric(0.7), top_n = as.integer(3L)) %&gt;% unlist()
#   
#   }
#   
#   
#   
#   res2 &lt;- purrrgress::pro_map(titles$value, similar) %&gt;% rlist::list.cbind() %&gt;% as.data.frame()
#   
#   
#   sims &lt;- res2 %&gt;% tidytable::pivot_longer.() %&gt;% mutate(check=as.numeric(value)) %&gt;% filter(!is.na(check))
#   
#   key &lt;- res2 %&gt;% tidytable::pivot_longer.() %&gt;% mutate(check=as.numeric(value)) %&gt;% filter(is.na(check))
#   
#   
#   keybert &lt;- data.table(keyword=key$value, cluster_similarity=sims$value, sentence=as.numeric(str_replace(key$name, &quot;V&quot;,&quot;&quot;))) %&gt;% distinct(keyword, sentence)%&gt;% inner_join.(titles %&gt;% mutate(sentence=row_number()))
#   
#   
#   
#   
#   
#   
# })
# 
# while(exists(&quot;keybert&quot;)==F) {
#   Sys.sleep(3)
# }

keybert &lt;- fst::read_fst(&quot;/home/knut/Desktop/keybert.fst&quot;)

keybert_counts &lt;- keybert %&gt;% group_by(keyword) %&gt;% count()


# f &lt;- callr({
#   library(spacyr)
# 
#   
#   spacy_initialize(model = &quot;en_core_web_trf&quot;, python_executable = &quot;/home/knut/transformers/bin/python3&quot;)
# 
#   tags &lt;- spacy_parse(keybert_counts$keyword, entity = T, nounphrase = T)
#   
#   
# }, workers=1)


# pos_tags_keybert &lt;- value(f)

pos_tags_keybert &lt;- fst::read_fst(&quot;/home/knut/Desktop/keybert_pos_tags.fst&quot;)


keybert_counts %&gt;% ungroup() %&gt;% sample_n(100)</code></pre>
<pre><code>## # A tibble: 100 × 2
##    keyword                          n
##    &lt;chr&gt;                        &lt;int&gt;
##  1 run schedule python              1
##  2 overcoming imposter syndrome     1
##  3 robust decision making           1
##  4 matches pymc3                    1
##  5 sentiment reddit subgroup        1
##  6 google search packages           1
##  7 python altair combines           1
##  8 image processing hackathon       1
##  9 interview ai                     1
## 10 shopping                         1
## # … with 90 more rows</code></pre>
<p>Next we want to make them comparable with word embeddings. Some of these phrases are pieces of sentences, yet, I’m first interested in words and keywords, because those yield a general and easiest to grasp grid to traverse.<br />
Thus, I choose non-contextual word embeddings for this task. I will use TransE embeddings trained on the CommonSense Knowledge Graph (CSKG, see <a href="https://github.com/usc-isi-i2/cskg" class="uri">https://github.com/usc-isi-i2/cskg</a>) to initialize doc2vec on both titles and blog posts themselves for quick hack to a complete, general grid of words with domain idiosyncrasies.</p>
<pre class="r"><code>#prepare the embeddings 

CSKG_labels &lt;- fread(&quot;/home/knut/nlpaug/cskg.tsv&quot;) %&gt;% select(`node1;label`, node1, `node2;label`, node2)
CSKG_labels &lt;- CSKG_labels %&gt;% distinct(`node1;label`, node1) %&gt;%
  rename(label=`node1;label`, node=node1) %&gt;% 
  bind_rows.(CSKG_labels %&gt;% distinct(`node2;label`, node2)%&gt;%
               rename(label=`node2;label`, node=node2)) %&gt;% distinct.()

labels &lt;- splitstackshape::cSplit(CSKG_labels, &quot;label&quot;, &quot;|&quot;)
labels &lt;- labels %&gt;% melt(id.vars=&quot;node&quot;, na.rm = T)


CSKG_embs &lt;- fread(&quot;/home/knut/nlpaug/embs/trans_log_dot_0.1.tsv&quot;) %&gt;% inner_join.(labels %&gt;% rename(V1=node, label=value) %&gt;% select(-variable))


CSKG_embs &lt;- CSKG_embs %&gt;% select.(-V1) %&gt;% distinct.()
names1 = make.names(CSKG_embs$label, unique=TRUE) %&gt;% str_replace_all(&quot;[.]&quot;, &quot;_&quot;) %&gt;% make.names(unique = T)

rownames(CSKG_embs) &lt;-  names1

CSKG_embs &lt;- CSKG_embs %&gt;% select.(-label)




#merge titles and posts for doc2vec 

paragraphs &lt;- blogposts %&gt;% filter.(variable==&quot;paragraphs&quot;) %&gt;% distinct(value) %&gt;% pull(value)

x &lt;- data.table(original=c(titles$value, paragraphs) %&gt;% unique()) %&gt;% mutate(doc_id=row_number())


ngrams &lt;- x %&gt;% unnest_ngrams(output = &quot;ngram&quot;, input = &quot;original&quot;, n = 3, n_min = 2, ngram_delim = &quot;_&quot;, drop = F)



#doc2vec for word embeddings

x$text   &lt;- tolower(x$original)
x$text   &lt;- gsub(&quot;[^[:alpha:]]&quot;, &quot; &quot;, x$text)
x$text   &lt;- gsub(&quot;[[:space:]]+&quot;, &quot; &quot;, x$text)
x$text   &lt;- str_squish(x$text)
x$nwords &lt;- txt_count_words(x$text)
x$text &lt;- stringr::word(x$text, start = 1, end = ifelse(x$nwords&lt;1000, x$nwords, 999)) 
x &lt;- na.omit(x)


xs &lt;- subset(x, nwords &gt; 1 &amp; nchar(text) &gt; 0)


#replace ngram tokens

library(ngram)
bigrams &lt;- ngram(xs$text, n=2)
bigrams &lt;- get.phrasetable(bigrams)
bigrams$ngrams &lt;- str_replace_all(bigrams$ngrams %&gt;% str_squish(), &quot; &quot;, &quot;_&quot;)
bigrams &lt;- inner_join.(bigrams, data.table(ngrams=names1))

xs &lt;- subset(xs, nwords &gt; 2 &amp; nchar(text) &gt; 0)
trigrams &lt;- ngram(xs$text, n=3)
trigrams &lt;- get.phrasetable(trigrams)
trigrams$ngrams &lt;- str_replace_all(trigrams$ngrams %&gt;% str_squish(), &quot; &quot;, &quot;_&quot;)
trigrams &lt;- inner_join.(trigrams, data.table(ngrams=names1))


Replaces &lt;- data.frame(from = c(bigrams$ngrams %&gt;% str_replace_all(&quot;_&quot;, &quot; &quot;), trigrams$ngrams %&gt;% str_replace_all(&quot;_&quot;, &quot; &quot;)), to = c(bigrams$ngrams, trigrams$ngrams))

keyword_replaces &lt;- keyw_adjective %&gt;% filter.(ngram&gt;1) %&gt;% mutate(from=title, to=str_replace_all(title, &quot; &quot;, &quot;_&quot;)) %&gt;% select.(from, to) %&gt;% 
  bind_rows.(keyw_nouns %&gt;% filter.(ngram&gt;1) %&gt;% mutate(from=title, to=str_replace_all(title, &quot; &quot;, &quot;_&quot;)) %&gt;% select.(from, to), 
             keyw_verbs %&gt;% filter.(ngram&gt;1) %&gt;% mutate(from=title, to=str_replace_all(title, &quot; &quot;, &quot;_&quot;)) %&gt;% select.(from, to), 
             keybert_counts %&gt;% mutate(from=keyword, to=str_replace_all(keyword, &quot; &quot;, &quot;_&quot;)) %&gt;% select.(from, to)
    
  )


all_replacements &lt;- Replaces %&gt;% bind_rows.(keyword_replaces)



# library(disk.frame)
# # this will set up disk.frame with multiple workers
# setup_disk.frame()
# # this will allow unlimited amount of data to be passed from worker to worker
# options(future.globals.maxSize = Inf)
# 
# 
# text_df &lt;- as.disk.frame(x, outdir = &quot;/home/knut/Desktop/text_df/&quot;, overwrite = T)


library(tidytext)
unnest_unigrams&lt;-function(chunk){tidytext::unnest_ngrams(tbl = chunk, output=&quot;ngram&quot;, input=&quot;text&quot;, drop=F, n=1L)}
unnest_bigrams&lt;-function(chunk){tidytext::unnest_ngrams(tbl = chunk, output=&quot;ngram&quot;, input=&quot;text_clean&quot;, drop=F, n=2L)}
unnest_trigrams&lt;-function(chunk){tidytext::unnest_ngrams(tbl = chunk, output=&quot;ngram&quot;, input=&quot;text&quot;, drop=F, n=3L)}

#extract ngram tokens of knowledge graph

#not all of this code is at interactive data analysis speed, so I comment it out and use stored objects

# # text_df_unigrams&lt;-text_df %&gt;% cmap(unnest_unigrams) 
# # text_df_unigrams %&gt;% write_disk.frame(outdir  = &quot;/home/knut/Desktop/text_df_unigrams/&quot;, overwrite = T, compress = 100)
# text_df_unigrams &lt;- disk.frame(&quot;/home/knut/Desktop/text_df_unigrams/&quot;) %&gt;% collect()
# text_df_unigrams &lt;- text_df_unigrams %&gt;% mutate.(id=row_number.())
# 
# 
# 
# 
# 
# # text_df_trigrams&lt;-text_df %&gt;% cmap(unnest_trigrams) 
# # text_df_trigrams %&gt;% write_disk.frame(outdir  = &quot;/home/knut/Desktop/text_df_trigrams/&quot;, overwrite = T, compress = 100)
# # text_df_trigrams &lt;- disk.frame(&quot;/home/knut/Desktop/text_df_trigrams/&quot;)
# # text_df_trigrams &lt;- shard(df = text_df_trigrams, shardby = &quot;text&quot;, nchunks = 900, overwrite = T)
# # text_df_trigrams %&gt;% write_disk.frame(outdir  = &quot;/home/knut/Desktop/text_df_trigrams/&quot;, overwrite = T, compress = 100)
# 
# 
# # attach all keyword ngrams
# text_df_trigrams&lt;-disk.frame(&quot;/home/knut/Desktop/text_df_trigrams/&quot;) %&gt;% left_join(all_replacements %&gt;% rename(ngram=from), by=c(&quot;ngram&quot;=&quot;ngram&quot;)) %&gt;% collect() # %&gt;% mutate(ngram=ifelse(!is.na(to), to, ngram)) %&gt;% select(c(&quot;text&quot;, &quot;ngram&quot;)) %&gt;% collect() 
# 
# 
# text_df_trigrams2 &lt;- text_df_trigrams %&gt;% filter.(str_detect(to, &quot;_&quot;)) %&gt;% distinct.()
# 
# text_df_trigrams2 &lt;- text_df_trigrams2 %&gt;% unnest_tokens(output = &quot;unigram&quot;, input = &quot;ngram&quot;, drop = F) %&gt;% inner_join.(text_df_unigrams %&gt;% select.(-original, -nwords, -doc_id), by=c(&quot;text&quot;=&quot;text&quot;, &quot;unigram&quot;=&quot;ngram&quot;)) 
# 
# text_df_trigrams2 &lt;- text_df_unigrams %&gt;% left_join.(text_df_trigrams2 %&gt;% select.(text, doc_id, ngram=unigram, to))
# 
# #text_df_trigrams %&gt;% write_disk.frame(outdir = &quot;/home/knut/Desktop/text_df_trigrams2/&quot;, overwrite = T) 
# 
# 
# 
# # 
# # 
# # text_df_bigrams&lt;-disk.frame(&quot;/home/knut/Desktop/text_df_trigrams2/&quot;) %&gt;% cmap(unnest_bigrams) 
# # text_df_bigrams %&gt;% write_disk.frame(outdir  = &quot;/home/knut/Desktop/bigrams_twds/&quot;, overwrite = T, compress = 100)
# # text_df_bigrams &lt;- disk.frame(&quot;/home/knut/Desktop/bigrams_twds/&quot;)
# # text_df_bigrams = shard(text_df_bigrams, shardby = &quot;text_clean&quot;, outdir = &quot;/home/knut/Desktop/bigrams_twds2/&quot;, nchunks = 900, overwrite = T)
# 
# 
# 
# text_df_bigrams&lt;-disk.frame(&quot;/home/knut/Desktop/bigrams_twds/&quot;) %&gt;% collect()
# 
# 
# text_df_bigrams &lt;- text_df_bigrams %&gt;% left_join.(all_replacements %&gt;% rename(ngram=from), by=c(&quot;ngram&quot;=&quot;ngram&quot;))
# 
# 
# text_df_bigrams2 &lt;- text_df_bigrams %&gt;% filter.(str_detect(to, &quot;_&quot;)) %&gt;% distinct.()
# text_df_bigrams2 &lt;- text_df_unigrams %&gt;% left_join.(text_df_bigrams2 %&gt;% select.(text, ngram, to))
# 
# text_df_bigrams2 &lt;- text_df_bigrams2 %&gt;% unnest_tokens(output = &quot;unigram&quot;, input = &quot;ngram&quot;, drop = F) %&gt;% inner_join.(text_df_unigrams %&gt;% select.(-original, -nwords, -doc_id), by=c(&quot;text&quot;=&quot;text&quot;, &quot;unigram&quot;=&quot;ngram&quot;)) 
# 
# text_df_trigrams3 &lt;- text_df_trigrams2 %&gt;% left_join.(text_df_bigrams2 %&gt;% select.(text, unigram, bigram=to), by=c(&quot;text&quot;=&quot;text&quot;, &quot;ngram&quot;=&quot;unigram&quot;))
# 
# text_df_trigrams3 &lt;- text_df_trigrams3 %&gt;% distinct.()
# 
# 
# 
# 
# #now reduce bigrams and trigrams
# 
# text_df_trigrams3&lt;-text_df_trigrams3 %&gt;% mutate(ngram=ifelse(!is.na(bigram), bigram, ngram)) %&gt;% mutate(ngram=ifelse(!is.na(to), to, ngram)) %&gt;% select.(-to, -bigram) %&gt;% distinct.()
# text_df_trigrams3 &lt;- text_df_trigrams3 %&gt;% select.(-id) %&gt;% distinct.()
# 
# 
# #reconstruct data with ngrams
# 
# 
# doc2vecdata &lt;- text_df_trigrams3 %&gt;% 
#      summarise.(text_ngrams = str_c(ngram, collapse = &quot; &quot;), .by = c(original, text, doc_id))
# 
# 
# doc2vecdata$word_count &lt;- stringi::stri_count_words(doc2vecdata$text)
# 
# 
# doc2vecdata &lt;- doc2vecdata %&gt;% filter.(word_count&gt;4)



#finally got the trainingdata 

# d &lt;- doc2vecdata %&gt;% select.(doc_id, text=text_ngrams)

d &lt;- fst::read_fst(&quot;/home/knut/Desktop/doc2vectraindata.fst&quot;)

#rm(text_df_bigrams, ngrams, text_df_trigrams, toks_ngram, text_df_trigrams2, text_df_bigrams2)
gc()</code></pre>
<pre><code>##             used   (Mb) gc trigger    (Mb)   max used    (Mb)
## Ncells  32188089 1719.1   87129625  4653.3   77078522  4116.5
## Vcells 737929762 5630.0 1812531024 13828.6 2265438778 17284.0</code></pre>
<pre class="r"><code>CSKG_embs2 &lt;- CSKG_embs %&gt;% as.matrix()
rownames(CSKG_embs2) &lt;- rownames(CSKG_embs)

doc2vec_CSKG_embs &lt;- paragraph2vec(d, type = &quot;PV-DM&quot;, threads = 15, embeddings = CSKG_embs2, dim = ncol(CSKG_embs), iter = 10, window = 10L, min_count = 3)

doc2vec_CSKG_words &lt;- as.matrix(doc2vec_CSKG_embs, which = &quot;words&quot;)



words &lt;- data.table(words=rownames(doc2vec_CSKG_words), id=1:length(rownames(doc2vec_CSKG_words))) %&gt;% filter(str_detect(words, &quot;python&quot;))


targets &lt;- doc2vec_CSKG_words[words$id,]

examples &lt;- paragraph2vec_similarity(targets, doc2vec_CSKG_words, top_n = 5) %&gt;% filter.(similarity&gt;0.7) %&gt;% filter.(term1!=term2)

examples</code></pre>
<pre><code>## # A tidytable: 171 × 4
##    term1                 term2                  similarity  rank
##    &lt;chr&gt;                 &lt;chr&gt;                       &lt;dbl&gt; &lt;dbl&gt;
##  1 well_python           python_well                 0.946     2
##  2 well_python           know_python                 0.719     3
##  3 using_python_selenium scraping_using_python       0.731     2
##  4 using_python_machine  anisdismail                 0.820     2
##  5 using_python_machine  gsitechnology               0.814     3
##  6 using_python_machine  dotson                      0.812     4
##  7 using_python_machine  c_o_d                       0.811     5
##  8 tips_python           python_tips                 0.968     2
##  9 tips_python           simple_tips                 0.781     3
## 10 tips_python           ipython_magic_commands      0.742     4
## # … with 161 more rows</code></pre>
<p>Check out this graph plot here with similar terms for the word Python:</p>
<pre class="r"><code>node_labels &lt;- c(examples$term1, examples$term2) %&gt;% unique()

node_ids &lt;- 1:length(node_labels)




nodes &lt;- data.frame(id = node_ids,
                    
  # add labels on nodes
  label = node_labels,
  
  
  # size adding value
  #value = 1,          
  


  # control shape of nodes
  shape = rep(&quot;box&quot;, length(node_labels)))#,
  


relations_filtered_ids &lt;- examples %&gt;% inner_join.(nodes %&gt;% rename(term1=label)) 
relations_filtered_ids2 &lt;- examples %&gt;% inner_join.(nodes %&gt;% rename(term2=label)) 




edges &lt;- data.frame(from = relations_filtered_ids$id, 
                    to = relations_filtered_ids2$id, label=&quot;similarity&quot;)


library(tidygraph)

g &lt;- tbl_graph(nodes = nodes, edges = edges, directed = FALSE)

g &lt;- g %&gt;% 
  activate(nodes) %&gt;% 
  mutate(degree = centrality_degree(), betweenness = centrality_betweenness()) %&gt;% 
  activate(edges) %&gt;% 
  mutate(betweenness = centrality_edge_betweenness()) %&gt;% 
  arrange(betweenness)


nodes2 &lt;- activate(g, nodes) %&gt;% as_tibble() %&gt;% mutate(value=degree*betweenness)





library(visNetwork)

visNetwork(nodes2, edges) %&gt;% 
   visOptions(highlightNearest = list(enabled = T, hover = F))</code></pre>
<div id="htmlwidget-1" style="width:672px;height:480px;" class="visNetwork html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"nodes":{"id":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167],"label":["well_python","using_python_selenium","using_python_machine","tips_python","tensorflow_python","scratch_using_python","scratch_python","scraping_using_python","scikit_learn_python","reddit_python","realpythonproject","pythonlevel","pythondatasciencehandbook","pythonanywhere","python_well","python_visualization_panel","python_visualization","python_virtual_environment","python_using_machine","python_tricks","python_tips","python_tensorflow","python_seaborn","python_scripts","python_script","python_scratch","python_python","python_projects","python_program","python_plotly","python_pipeline","python_pandas","python_packages","python_opencv","python_oop","python_notebook","python_nlp","python_model","python_matplotlib","python_map","python_lists","python_list","python_library","python_libraries","python_learn","python_it","python_introduction","python_helped","python_guide","python_functions","python_feature","python_exploratory_data","python_example","python_environment","python_elasticsearch","python_dictionary","python_dictionaries","python_decorators","python_data_analytics","python_data_analysis","python_data","python_dash","python_class","python_best_practices","python_applications","python_analysis","python_algorithm","python_a_guide","python","plotly_python","pipeline_python","pandas_python","opencv_python","oop_python","nlp_python","new_python_environment","model_python","map_python","manage_python","lists_python","learning_python","learn_python_data","learn_python","language_python","know_python","ipython_magic_commands","introduction_python","install_python","guide_python","feature_python","example_python","elasticsearch_python","dictionaries_python","decorators_python","data_python","dash_python","cool_python_libraries","better_python_programmer","applications_python","analysis_python","algorithm_python","a_python_package","a_python_library","anisdismail","gsitechnology","dotson","c_o_d","simple_tips","data_science_introduction","scratch_using","scikit_learn_library","praw","boyangzhao","appsilon","wildregressor","opensourceoptions","advancedtype","jakevdp","niyogi","hervé","netlify","visualization_panel_app","visualization_package","virtual_environment","pipenv","venv","predict_student_grades","writing_advanced_sql","scripts","shell_commands","script","geoviews","packages","libraries","in_libraries","imutils","jupyter_notebook","library","package","treehouse","functions","decorators","data_using_pycaret","a_dictionary","dictionary","data_analysis_library","plotly_dash","interactive_visualizations","dataclass","fbdea","fbddb","dunson","leiserson","abdel","andrade","julia","programming_language","cython","runserver","julia_language","install","install_tensorflow","awesomedata","idan","manyika","a_library","a_package"],"shape":["box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box","box"],"degree":[4,2,4,7,2,1,2,2,1,1,4,1,3,1,4,1,7,5,1,2,6,2,2,2,3,2,2,1,2,5,2,2,6,3,2,1,2,2,2,2,4,4,4,5,2,2,2,1,2,2,2,1,2,1,2,2,2,2,2,1,2,4,1,4,2,2,2,4,5,4,2,2,2,2,2,2,2,2,1,4,2,2,4,3,4,6,2,2,2,2,2,2,2,2,2,3,4,2,2,2,2,4,5,1,3,1,1,3,3,1,1,1,1,1,1,2,1,1,1,1,1,1,1,2,1,1,1,1,1,1,1,1,2,2,2,1,1,1,1,1,1,1,1,1,1,1,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,1],"betweenness":[0,0,27,603,14,0,378,306,0,0,470,0,30,0,444.5,0,450.5,297,0,44,745,126,12,25,200.5,86,10,0,22,60,42,146.5,314,160,82,0,40,40,59,59,82,245,500,282.5,86,86,86,0,12,4,10,0,24,0,24,44,61.5,61.5,44,0,18,34,0,27,12,12,12,3,42,138,38.5,42,42,42,42,42,42,1,0,42.5,21,21,42,87,339,170,44,1,21.5,21.5,44,44,44,44,1,11,6,3,3,6,1,24,24,0,86.5,0,0,24,34,0,0,0,0,0,0,0,0,0,0,0,0,0,0,20.5,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"value":[0,0,108,4221,28,0,756,612,0,0,1880,0,90,0,1778,0,3153.5,1485,0,88,4470,252,24,50,601.5,172,20,0,44,300,84,293,1884,480,164,0,80,80,118,118,328,980,2000,1412.5,172,172,172,0,24,8,20,0,48,0,48,88,123,123,88,0,36,136,0,108,24,24,24,12,210,552,77,84,84,84,84,84,84,2,0,170,42,42,168,261,1356,1020,88,2,43,43,88,88,88,88,2,33,24,6,6,12,2,96,120,0,259.5,0,0,72,102,0,0,0,0,0,0,0,0,0,0,0,0,0,0,41,0,0,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]},"edges":{"from":[1,1,2,3,3,3,3,4,4,4,4,5,6,7,8,9,10,11,11,11,11,12,13,13,13,14,15,15,16,17,17,17,17,18,18,18,18,19,20,21,21,21,21,22,23,23,24,24,25,25,26,27,28,29,30,30,30,31,32,33,33,33,33,34,34,35,36,37,38,39,40,41,41,42,42,43,43,43,44,44,44,44,45,46,47,48,49,50,50,51,52,53,54,55,56,56,57,58,59,60,61,62,62,62,63,64,64,64,64,65,66,67,68,68,68,68,69,69,69,69,70,70,71,72,73,74,75,76,77,78,79,80,80,81,82,83,83,84,84,85,85,86,86,86,86,87,88,88,89,90,91,92,93,94,95,96,96,97,97,97,97,98,99,100,101,102,102,102,103,103,103],"to":[1,1,2,4,4,4,5,7,8,15,15,17,17,17,18,20,21,21,22,25,26,27,29,30,30,31,32,33,33,34,35,37,38,39,40,41,41,42,42,43,44,45,46,47,49,51,53,55,57,58,59,61,62,65,66,67,69,70,70,71,72,73,74,75,76,77,78,80,80,81,82,83,83,84,85,85,86,86,87,89,90,91,92,93,94,95,96,98,99,100,101,102,103,103,104,105,105,105,106,107,108,108,108,109,109,109,110,111,112,113,114,115,116,116,117,118,119,120,121,122,123,124,124,125,126,127,128,129,130,131,132,133,133,134,134,135,135,136,137,138,139,140,141,142,143,144,145,146,147,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,166,167],"label":["similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity","similarity"]},"nodesToDataframe":true,"edgesToDataframe":true,"options":{"width":"100%","height":"100%","nodes":{"shape":"dot"},"manipulation":{"enabled":false}},"groups":null,"width":null,"height":null,"idselection":{"enabled":false,"style":"width: 150px; height: 26px","useLabels":true,"main":"Select by id"},"byselection":{"enabled":false,"style":"width: 150px; height: 26px","multiple":false,"hideColor":"rgba(200,200,200,0.5)","highlight":false},"main":null,"submain":null,"footer":null,"background":"rgba(0, 0, 0, 0)","highlight":{"enabled":true,"hoverNearest":false,"degree":1,"algorithm":"all","hideColor":"rgba(200,200,200,0.5)","labelOnly":true},"collapse":{"enabled":false,"fit":false,"resetHighlight":true,"clusterOptions":null,"keepCoord":true,"labelSuffix":"(cluster)"}},"evals":[],"jsHooks":[]}</script>
<p>Now the same again for my pretrained fasttext embeddings. I felt like buying the Pastukhov databases for an affordable 500 euro and trained an ngram fasttext model on 1.3 billion English google queries. For the query data science, reasonable similar bigrams are suggested below:</p>
<pre class="r"><code>library(fastTextR)
library(wordVectors)

model &lt;- fastTextR::ft_load(&quot;/run/media/knut/HD/MLearningAlgoTests/data/Pastukhov_clean_english.bin&quot;)

ft_embs &lt;- ft_word_vectors(model, all_replacements$from) %&gt;% as.matrix() %&gt;% unique() %&gt;% as.VectorSpaceModel()


wordVectors::closest_to(matrix = ft_embs, ft_embs[[&quot;data science&quot;]], n = 1000) %&gt;% as.data.frame()  %&gt;% inner_join.(bigrams %&gt;% mutate(word=str_replace_all(ngrams, &quot;_&quot;, &quot; &quot;))) %&gt;% filter.(freq&gt;3)%&gt;% mutate(interest=`similarity to ft_embs[[&quot;data science&quot;]]`*prop) %&gt;% arrange.(desc(interest)) %&gt;% select.(-ngrams)</code></pre>
<pre><code>## # A tidytable: 27 × 5
##    word                `similarity to ft_embs[[&quot;data sc…`  freq    prop interest
##    &lt;chr&gt;                                            &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;    &lt;dbl&gt;
##  1 data science                                     1     32227 9.34e-4  9.34e-4
##  2 data scientist                                   0.801 11006 3.19e-4  2.55e-4
##  3 computer science                                 0.766  1862 5.39e-5  4.13e-5
##  4 science project                                  0.764  1081 3.13e-5  2.39e-5
##  5 in science                                       0.782   230 6.66e-6  5.21e-6
##  6 science research                                 0.746   110 3.19e-6  2.38e-6
##  7 network science                                  0.798    38 1.10e-6  8.78e-7
##  8 information science                              0.756    36 1.04e-6  7.89e-7
##  9 new science                                      0.810    28 8.11e-7  6.57e-7
## 10 applied science                                  0.757    24 6.95e-7  5.26e-7
## # … with 17 more rows</code></pre>
<p>Projectiles have been spawned.</p>
<p>Gotta bounce.</p>
<div class="figure">
<img src="flakcanon.jpg" alt="" />
<p class="caption">ss</p>
</div>
<style type="text/css">

@import url(https://fonts.googleapis.com/css?family=Open+Sans:wght@300);

body{ /* Normal  */
      font-size: 16px;
      font-family: "Open Sans";
  }

td {  /* Table  */
  font-size: 8px;
}
h1.title {
  font-size: 20px;
  color: #324c63;
}
h1 { /* Header 1 */
  font-size: 30px;
  font-family: "Roboto";
}
h2 { /* Header 2 */
    font-size: 26px;
    font-family: "Roboto";
}
h3 { /* Header 3 */
  font-size: 22px;
  font-family: "Roboto";
}
code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 14px;
    color: white;
    background-color: black;
}

p { /* Code block - determines code spacing between lines */
    font-size: 18px;
    font-family: "Open Sans";
    margin: 0px 0px 30px;
    padding: 10px;
    line-height: 200%;
}
</style>
</div>
